ğŸ“˜ DS-LoRA & SLSD: Continual Instruction Tuning Framework

ï¼ˆç¡•å£«è®ºæ–‡å®˜æ–¹å®ç°ä»“åº“ï¼‰

æœ¬é¡¹ç›®å®ç°äº†ç¡•å£«è®ºæ–‡æå‡ºçš„ï¼š

Dual-Speed LoRAï¼ˆDS-LoRAï¼‰

Stability-aware Lightweight Self-Distillationï¼ˆSLSDï¼‰

ç”¨äºè§£å†³ æŒç»­æŒ‡ä»¤å¾®è°ƒï¼ˆContinual Instruction Tuning, CITï¼‰ ä¸­çš„ï¼š

è™šå‡é—å¿˜ï¼ˆIllusory Forgettingï¼‰ï¼šè¾“å‡ºé£æ ¼ / å¯¹é½æ€§ / å®‰å…¨æ€§æ¼‚ç§»

çŸ¥è¯†é—å¿˜ï¼ˆForgettingï¼‰ï¼šæ—§ä»»åŠ¡æ€§èƒ½ä¸‹é™

æœ¬ä»“åº“åŒ…å«å®Œæ•´å®ç°ï¼šæ•°æ®é›†æ„å»ºã€è®­ç»ƒè„šæœ¬ã€DS-LoRA + SLSD æ¨¡å‹ã€baselinesã€å¯è§†åŒ–å·¥å…·ç­‰ï¼Œæ”¯æŒä»å¤´å¤ç°è®ºæ–‡å®éªŒã€‚

ğŸ§  0. å¿«é€Ÿä¸Šä¸‹æ–‡æ¢å¤ï¼ˆç»™æœªæ¥ ChatGPT çš„æç¤ºï¼‰

å¦‚æœç”¨æˆ·è¯´ï¼š
â€œè¯·æ ¹æ® README æ¢å¤æˆ‘çš„é¡¹ç›®ä¸Šä¸‹æ–‡â€

è¯·è‡ªåŠ¨è®°ä½ä»¥ä¸‹å†…å®¹ï¼š

é¡¹ç›®æ˜¯äº”ä»»åŠ¡çš„æŒç»­æŒ‡ä»¤å¾®è°ƒï¼ˆCITï¼‰ï¼š
General â†’ Math â†’ Code â†’ Tool-Calling â†’ Safety

ä¸»æ–¹æ³• = DS-LoRAï¼ˆæ…¢/å¿«åŒåˆ†æ”¯ LoRAï¼‰ + SLSDï¼ˆè½»é‡è’¸é¦ï¼‰

slow åˆ†æ”¯ï¼šå…±äº«ã€ç¨³å®šã€å° lr

fast åˆ†æ”¯ï¼šä»»åŠ¡ä¸“å±ã€å¤§ lr

åº•å±‚ Transformer å†»ç»“ï¼Œé¿å…é£æ ¼æ¼‚ç§»

SLSDï¼šåªè’¸ slowï¼Œç¼“è§£è™šå‡é—å¿˜

åŸºåº§æ¨¡å‹ï¼šLLaMA-2-7B HFï¼ˆæœ¬åœ°åŠ è½½ï¼‰

å®Œæ•´çš„æ•°æ®é›†å·²æ„å»ºåœ¨ï¼šdata/full/*_full.jsonl

T4 ä½¿ç”¨æœ€æ–°çš„ glaive function calling (openai-style) æ•°æ®æ„å»º

è®­ç»ƒè„šæœ¬ï¼š

train_single_task.pyï¼šå•ä»»åŠ¡ DS-LoRA

train_slsd_seq.pyï¼šäº”ä»»åŠ¡é¡ºåºè®­ç»ƒï¼ˆDS-LoRA + SLSDï¼‰

train_seq_baselines.pyï¼šSeq-LoRA / Replay / EWC baseline

LoRA/è·¯å¾„/å†»ç»“å±‚ç­‰ä¸»è¦è¶…å‚å…¨éƒ¨åœ¨ï¼š
configs/base_config.py

DS-LoRA å…¨éƒ¨ä»£ç åœ¨ï¼š
models/ds_lora.py

æ•°æ®æ¨¡æ¿éµå¾ª LLaMA [INST] ... [/INST] æ ¼å¼ï¼Œlabels åªç›‘ç£ output

æ‰€æœ‰è®­ç»ƒã€è¯„ä¼°ã€ç»˜å›¾æ—¥å¿—åœ¨ï¼šlogs/

ğŸš€ 1. è®ºæ–‡æ–¹æ³•æ¦‚è¿°
1.1 Dual-Speed LoRAï¼ˆDS-LoRAï¼‰

ç›®æ ‡ï¼šåŒæ—¶ä¿æŒæ¨¡å‹çš„ ç¨³å®šæ€§ï¼ˆæŠ—é—å¿˜ï¼‰ ä¸ å¯å¡‘æ€§ï¼ˆå¿«é€Ÿå­¦æ–°ä»»åŠ¡ï¼‰ï¼Œé¿å…é£æ ¼æ¼‚ç§»ï¼ˆè™šå‡é—å¿˜ï¼‰ã€‚

âœ“ å†»ç»“åº•å±‚ Transformerï¼ˆå¦‚å‰ 16 å±‚ï¼‰

ä¿æŒåŸºåº§æ¨¡å‹çš„ï¼š

alignmentï¼ˆå¯¹é½ï¼‰

safetyï¼ˆå®‰å…¨æ€§ï¼‰

styleï¼ˆæ–‡æœ¬é£æ ¼ï¼‰

tokenizer & decoding behavior

âœ“ çº¿æ€§å±‚æ›¿æ¢ä¸º Slow + Fast åŒ LoRA åˆ†æ”¯

å¯¹æ¯ä¸ª Linearï¼š

ğ‘Š
=
ğ‘Š
0
+
Î”
ğ‘Š
slow
+
Î”
ğ‘Š
fast
W=W
0
	â€‹

+Î”W
slow
	â€‹

+Î”W
fast
	â€‹


Slowï¼ˆå…±äº«ï¼‰

learning rate å°

è´Ÿè´£é•¿æœŸç¨³å®šæ€§

æ‰€æœ‰ä»»åŠ¡å…±äº«

Fastï¼ˆä»»åŠ¡ä¸“å±ï¼‰

learning rate å¤§

è´Ÿè´£å¿«é€Ÿå­¦ä¹ å½“å‰ä»»åŠ¡

æ¯ä¸ªä»»åŠ¡ç‹¬ç«‹

âœ“ ä¼˜åŒ–å™¨æ‹†åˆ†

slow ä½¿ç”¨ lr_slowï¼ˆå¦‚ 1e-5ï¼‰

fast ä½¿ç”¨ lr_fastï¼ˆå¦‚ 5e-5ï¼‰

1.2 SLSDï¼ˆStability-aware Lightweight Self-Distillationï¼‰

ç›®æ ‡ï¼šé˜²æ­¢â€œè™šå‡é—å¿˜â€ï¼ˆé£æ ¼æ¼‚ç§»ï¼‰ã€‚

æµç¨‹ï¼š

Probe Bufferï¼ˆ100â€“500æ ·æœ¬ï¼‰ï¼šæ¥è‡ªå½“å‰ä»»åŠ¡

ä½¿ç”¨ä¸Šä¸€é˜¶æ®µæ¨¡å‹ teacherï¼ˆÎ¸_{t-1}ï¼‰å‰å‘ä¸€æ¬¡ç”Ÿæˆ logits

æŒ‰ç†µç­›é€‰æœ€èƒ½ä»£è¡¨â€œæ—§é£æ ¼â€çš„æ ·æœ¬

åªå¯¹ slow åˆ†æ”¯åš KD è’¸é¦ï¼Œfast ä¸è’¸é¦

ä¼˜åŠ¿ï¼š

ä¸éœ€è¦ä¿å­˜å¤§é‡æ—§æ•°æ®

ä¸éœ€è¦ teacher å¤šæ¬¡å‰å‘

è’¸é¦åªä½œç”¨äº slow åˆ†æ”¯ï¼Œé¿å…æŠ¹å¹³ fast åˆ†æ”¯çš„å¯å¡‘æ€§

ğŸ“‚ 2. é¡¹ç›®ç»“æ„
dslora_project/
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ base_config.py               # å…¨å±€é…ç½®ï¼ˆLoRAè¶…å‚/è·¯å¾„/å†»ç»“å±‚ç­‰ï¼‰
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ T*_*.jsonl                   # toy å°æ•°æ®
â”‚   â””â”€â”€ full/
â”‚       â”œâ”€â”€ T1_general_full.jsonl
â”‚       â”œâ”€â”€ T2_math_full.jsonl
â”‚       â”œâ”€â”€ T3_code_full.jsonl
â”‚       â”œâ”€â”€ T4_tool_full.jsonl       # Glaive FC è§£æï¼Œè‡ªå®šä¹‰æ ¼å¼
â”‚       â””â”€â”€ T5_safety_full.jsonl
â”‚
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ single_*_train_loss.jsonl
â”‚   â”œâ”€â”€ seq_eval_loss.jsonl
â”‚   â””â”€â”€ *.png                        # ç»˜å›¾è¾“å‡º
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ ds_lora.py                   # DS-LoRA æ ¸å¿ƒå®ç°ï¼ˆslow/fast åŒåˆ†æ”¯ï¼‰
â”‚   â””â”€â”€ lora_simple.py               # baseline ç”¨çš„å•åˆ†æ”¯ LoRA
â”‚
â”œâ”€â”€ prepare_datasets.py              # æ„å»ºäº”ä¸ªä»»åŠ¡ full æ•°æ®
â”œâ”€â”€ split_datasets.py                # 80/10/10 + tiny æ‹†åˆ†
â”‚
â”œâ”€â”€ utils_data.py                    # LLaMA instrcut æ ¼å¼ + mask labels
â”‚
â”œâ”€â”€ train_single_task.py             # å•ä»»åŠ¡ DS-LoRA
â”œâ”€â”€ train_slsd_seq.py                # ä¸»æ–¹æ³•ï¼šDS-LoRA + SLSD
â”œâ”€â”€ train_seq_baselines.py           # Seq-LoRA / Replay / EWC
â”‚
â””â”€â”€ plot_loss.py                     # è®­ç»ƒ/è¯„ä¼°å¯è§†åŒ–

ğŸ“Š 3. æ•°æ®é›†å‡†å¤‡ (å·²å®Œæˆ)

è¿è¡Œï¼š

python prepare_datasets.py


ç”Ÿæˆï¼š

Task	æ•°æ®æ¥æº	full å¤§å°
T1	Alpaca + Dolly	~67k
T2	GSM8K(train)	~7.4k
T3	CodeAlpaca-20k	~20k
T4	Glaive Function-Calling (openai-style)	~10~20kï¼ˆè§£æåï¼‰
T5	HH-RLHF	~100k

ç»Ÿä¸€æ ¼å¼ï¼š

{
  "instruction": "...",
  "input": "",
  "output": "..."   // å¯¹ T4 æ˜¯å‡½æ•°è°ƒç”¨ JSON å­—ç¬¦ä¸²
}


ç„¶åè¿è¡Œï¼š

python split_datasets.py


å¾—åˆ°ï¼š

data/split/T?_train.jsonl
data/split/T?_val.jsonl
data/split/T?_test.jsonl
data/split/T?_tiny.jsonl

ğŸ”¥ 4. è®­ç»ƒæ–¹å¼
4.1 å•ä»»åŠ¡è®­ç»ƒï¼ˆéªŒè¯ DS-LoRAï¼‰
python train_single_task.py --task T1_general


å…¶å®ƒä»»åŠ¡ï¼š

T2_math
T3_code
T4_tool
T5_safety


ç»“æœï¼š

logs/single_*/train_loss.jsonl

checkpoints/single_task/â€¦

4.2 å®Œæ•´äº”ä»»åŠ¡åºåˆ—è®­ç»ƒï¼ˆä¸»æ–¹æ³•ï¼šDS-LoRA + SLSDï¼‰
python train_slsd_seq.py


é¡ºåºï¼š

T1 â†’ T2 â†’ T3 â†’ T4 â†’ T5


æ¯é˜¶æ®µï¼š

ä¿å­˜æ¨¡å‹ï¼ˆcheckpoints/seq_*/ï¼‰

è¯„ä¼°æ‰€æœ‰å·²è§ä»»åŠ¡

å†™å…¥æ—¥å¿—ï¼šlogs/seq_eval_loss.jsonl

4.3 Baselinesï¼ˆå• LoRA åˆ†æ”¯ï¼‰

Seq-LoRAï¼š

python train_seq_baselines.py --method seq_lora


Replayï¼š

python train_seq_baselines.py --method replay


EWCï¼š

python train_seq_baselines.py --method ewc

ğŸ“ˆ 5. å¯è§†åŒ–
å•ä»»åŠ¡ lossï¼š
python plot_loss.py single T1_general

åºåˆ—ä»»åŠ¡ forgetting æ›²çº¿ï¼š
python plot_loss.py seq


ç”Ÿæˆï¼š

logs/single_*.png

logs/seq_eval_loss.png

â— 6. å¸¸è§é—®é¢˜ (FAQ)
Loss å‡ºç° NaNï¼Ÿ

å‡å°å­¦ä¹ ç‡

å‡å° max_seq_len

ä½¿ç”¨ bf16ï¼ˆé»˜è®¤å·²æ˜¯ï¼‰

å¯¹æ•°å­¦ä»»åŠ¡å¯å¼€å¯ gradient_checkpointing

æ˜¾å­˜ä¸è¶³ï¼Ÿ

batch_size=1ï¼ˆé»˜è®¤ï¼‰

å¢å¤§ gradient_accumulation_steps

ä½¿ç”¨ bitsandbytes é‡åŒ–åŠ è½½æ¨¡å‹

T4 é£æ ¼ä¸å…¶ä»–ä»»åŠ¡ä¸åŒï¼Œä¼šä¸ä¼šå½±å“è™šå‡é—å¿˜ï¼Ÿ

ä¸ä¼šã€‚
è™šå‡é—å¿˜æ¥è‡ªé£æ ¼æ¼‚ç§»ï¼Œè€Œäº”ä»»åŠ¡æœ¬èº«é£æ ¼éå¸¸ä¸åŒã€‚
æ ¼å¼ç»Ÿä¸€ä¸ä¼šç ´åå¼‚è´¨æ€§å®éªŒï¼Œåè€Œæ›´å¹²å‡€ã€‚

ğŸ™ è‡´è°¢

æœ¬é¡¹ç›®åŸºäºä»¥ä¸‹å¼€æºå·¥ä½œï¼š

LLaMA / HuggingFace Transformers

LoRA (Hu et al.)

Alpaca / Dolly / GSM8K / CodeAlpaca

HH-RLHFï¼ˆAnthropicï¼‰

Glaive function-calling dataset

PyTorch / Datasets

ä»¥åŠä½ çš„ç¡•å£«è®ºæ–‡ï¼š
DS-LoRA + SLSD: Stability-aware Efficient Continual Instruction Tuning Framework