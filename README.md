DS-LoRA & SLSD: Continual Instruction Tuning Framework è¯´æ˜æ–‡æ¡£
1. è®ºæ–‡ç›®æ ‡ä¸é—®é¢˜è®¾å®š
1.1 è®ºæ–‡ä¸»é¢˜

è®ºæ–‡é¢˜ç›®ï¼ˆæš‚å®šï¼‰

DS-LoRA + SLSD: Stability-aware Efficient Continual Instruction Tuning Framework

ç ”ç©¶ç›®æ ‡ï¼š
åœ¨ æŒç»­æŒ‡ä»¤å¾®è°ƒï¼ˆContinual Instruction Tuning, CITï¼‰ åœºæ™¯ä¸‹ï¼Œè®©ä¸€ä¸ª LLaMA-2-7B æŒ‡ä»¤æ¨¡å‹æŒ‰ä»»åŠ¡åºåˆ—ä¾æ¬¡å­¦ä¹ ï¼š

T1 General â†’ T2 Math â†’ T3 Code â†’ T4 Tool-Calling â†’ T5 Safety

åŒæ—¶å°½é‡é¿å…ä¸¤ç±»é—å¿˜ï¼š

çŸ¥è¯†é—å¿˜ï¼ˆCatastrophic Forgettingï¼‰

æ—§ä»»åŠ¡ä¸Šçš„å®¢è§‚æ€§èƒ½æ˜æ˜¾ä¸‹é™ï¼ˆå¦‚å‡†ç¡®ç‡ã€loss å˜å·®ï¼‰ã€‚

è™šå‡é—å¿˜ï¼ˆIllusory Forgettingï¼‰

æ¨¡å‹åœ¨æ—§ä»»åŠ¡ä¸Šè¿˜ â€œä¼šåšé¢˜â€ï¼Œä½†ï¼š

è¾“å‡ºé£æ ¼å˜äº†ï¼ˆå£å»ã€æ ¼å¼ä¸ä¸€è‡´ï¼‰

alignment å˜å·®

safety é™ä½ï¼ˆæ›´å®¹æ˜“ç»™å‡ºä¸å®‰å…¨å›ç­”ï¼‰

è®ºæ–‡æå‡ºä¸¤ä¸ªæ ¸å¿ƒæ–¹æ³•ï¼š

Dual-Speed LoRAï¼ˆDS-LoRAï¼‰ï¼šç»“æ„å±‚é¢ç¼“è§£é—å¿˜

Stability-aware Lightweight Self-Distillationï¼ˆSLSDï¼‰ï¼šè’¸é¦å±‚é¢ç¼“è§£â€œè™šå‡é—å¿˜â€

2. æ–¹æ³•æ¦‚è§ˆ
2.1 DS-LoRA

æ ¸å¿ƒæƒ³æ³•ï¼šåœ¨ LoRA ä¸Šåšâ€œå¿«æ…¢åŒåˆ†æ”¯â€ï¼Œå…¼é¡¾ ç¨³å®šæ€§ï¼ˆslowï¼‰ å’Œ å¯å¡‘æ€§ï¼ˆfastï¼‰ã€‚

å†»ç»“åº•å±‚ Transformerï¼Œå¤§è‡´åŒ…å«ï¼š

self-attn / FFN åŸå§‹æƒé‡

embedding / LM head ç­‰
â†’ ä¿æŒ alignment / safety / æ–‡æœ¬é£æ ¼ä¸æ¼‚ç§»ã€‚

å¯¹æ¯ä¸ªéœ€è¦é€‚é…çš„ Linear å±‚ï¼Œæ›¿æ¢ä¸ºï¼š

ğ‘Š
=
ğ‘Š
0
+
Î”
ğ‘Š
slow
+
Î”
ğ‘Š
fast
W=W
0
	â€‹

+Î”W
slow
	â€‹

+Î”W
fast
	â€‹


Slow åˆ†æ”¯ï¼š

æ‰€æœ‰ä»»åŠ¡å…±äº«

lr_slow å¾ˆå°ï¼ˆä¾‹å¦‚ 5e-6 / 1e-5ï¼‰

è´Ÿè´£é•¿æœŸç¨³å®šã€ç´¯ç§¯â€œé€šç”¨é£æ ¼â€å’ŒçŸ¥è¯†

Fast åˆ†æ”¯ï¼š

æ¯ä¸ªä»»åŠ¡ç‹¬ç«‹ LoRA

lr_fast è¾ƒå¤§ï¼ˆä¾‹å¦‚ 2e-5 / 5e-5ï¼‰

è´Ÿè´£å¿«é€Ÿé€‚åº”å½“å‰ä»»åŠ¡ç‰¹æ€§

è®­ç»ƒæ—¶ï¼š

Fast åˆ†æ”¯ï¼šä¸» supervised loss + å¤§å­¦ä¹ ç‡

Slow åˆ†æ”¯ï¼šä¸» supervised loss + SLSD è’¸é¦çº¦æŸï¼ˆå°å­¦ä¹ ç‡ï¼‰

2.2 SLSDï¼ˆStability-aware Lightweight Self-Distillationï¼‰

ç›®æ ‡ï¼šåœ¨ CIT ä¸­æŠ‘åˆ¶ â€œè™šå‡é—å¿˜â€ï¼ˆé£æ ¼æ¼‚ç§»ï¼‰ï¼Œä¸éœ€è¦ä¿å­˜å¤§é‡æ—§æ•°æ®ã€‚

è®­ç»ƒç¬¬ t ä¸ªä»»åŠ¡æ—¶ï¼š

ä½¿ç”¨ä¸Šä¸€é˜¶æ®µæ¨¡å‹ 
ğœƒ
ğ‘¡
âˆ’
1
Î¸
tâˆ’1
	â€‹

 ä½œä¸º teacherï¼›

åœ¨å½“å‰ä»»åŠ¡æ•°æ®ä¸Šæ„å»ºä¸€ä¸ª probe bufferï¼š

é‡‡æ ·å°‘é‡æ ·æœ¬ï¼ˆä¾‹å¦‚ 100â€“500 æ¡ï¼‰

è¿è¡Œ teacher å‰å‘ï¼Œè®¡ç®—æœ€åä¸€ä¸ª token çš„ é¢„æµ‹ç†µï¼›

åªä¿ç•™ç†µè¾ƒä½ï¼ˆteacher è‡ªä¿¡ï¼‰çš„æ ·æœ¬ï¼Œå¹¶ç¼“å­˜å…¶ logitsï¼›

åœ¨è®­ç»ƒå½“å‰ä»»åŠ¡æ—¶ï¼š

ä¸» supervised loss æ­£å¸¸æ›´æ–° slow + fastï¼›

æ¯éš”è‹¥å¹²æ­¥ï¼Œä» probe buffer æŠ½ä¸€ä¸ªæ ·æœ¬ï¼Œå¯¹å½“å‰æ¨¡å‹ logits ä¸ teacher logits åš KL è’¸é¦ï¼›

åªç”¨ slow åˆ†æ”¯çš„ optimizer æ›´æ–°ï¼ˆfast åˆ†æ”¯ä¸è¢« KDâ€œæŠ¹å¹³â€ï¼‰ã€‚

ä¼˜åŠ¿ï¼š

ä¸éœ€è¦å®Œæ•´ä¿ç•™æ—§ä»»åŠ¡æ•°æ®ï¼Œåªç”¨è½»é‡çš„ probe bufferï¼›

è’¸é¦åªçº¦æŸ slow åˆ†æ”¯ï¼Œå¯ä»¥ç¨³å®šé£æ ¼ã€å¯¹é½æ€§å’Œå®‰å…¨æ€§ï¼Œè€Œä¸å‰Šå¼± fast çš„å¯å¡‘æ€§ã€‚

3. å®éªŒè®¾ç½®ä¸æ•°æ®
3.1 æ¨¡å‹ä¸è®­ç»ƒè®¾å®š

åŸºåº§æ¨¡å‹ï¼šLLaMA-2-7Bï¼ˆHFï¼Œæœ¬åœ°è·¯å¾„åœ¨ BaseConfig.model_name ä¸­æŒ‡å®šï¼‰

è®­ç»ƒç±»å‹ï¼šCausal LMï¼ˆinstruction â†’ outputï¼‰

æ•°æ®æ ¼å¼ï¼ˆç»Ÿä¸€ JSONLï¼‰ï¼š

{"instruction": "...", "input": "...", "output": "..."}


åœ¨ InstructionDataset ä¸­ä¼šè¢«è½¬æ¢ä¸º LLaMA æŒ‡ä»¤æ ¼å¼ï¼š

[INST] instruction \n input [/INST] output </s>


labels åªç›‘ç£ output åŒºåŸŸï¼ˆprompt éƒ¨åˆ† label = -100ï¼‰ã€‚

3.2 ä»»åŠ¡ä¸æ•°æ®æ¥æº

æ•°æ®æ„å»ºè„šæœ¬ï¼šprepare_datasets.py â†’ ç”Ÿæˆ data/full/T?_*_full.jsonlï¼Œä¹‹åç”± split_datasets.py æ‹†åˆ†ã€‚

T1_generalï¼šAlpaca + Dolly ç±»é€šç”¨æŒ‡ä»¤

T2_mathï¼šGSM8K(train) é£æ ¼åˆ†æ­¥è§£é¢˜

T3_codeï¼šCodeAlpaca-20k é£æ ¼ä»£ç ç”Ÿæˆ/è§£é‡Š

T4_toolï¼šGlaive function-callingï¼ˆOpenAI style å‡½æ•°è°ƒç”¨ï¼‰ï¼Œè§£ææˆ JSON å­—ç¬¦ä¸²è¾“å‡º

T5_safetyï¼šHH-RLHF / å®‰å…¨å¯¹è¯æ•°æ®

3.3 æ•°æ®æ‹†åˆ†ä¸æ¸…æ´—

è„šæœ¬ï¼šsplit_datasets.pyï¼ˆæœ€æ–°ç‰ˆï¼Œå¸¦æ¸…æ´— + æ¯ä»»åŠ¡æœ€å¤§ 10K æ ·æœ¬ï¼‰

å¯¹æ¯ä¸ªä»»åŠ¡ï¼š

ä» data/full/{task}_full.jsonl è¯»å…¥

æ¸…æ´—é€»è¾‘ï¼š

å¿…é¡»åŒ…å« instruction / output å­—æ®µ

è½¬æˆå­—ç¬¦ä¸²å strip() ä¸èƒ½ä¸ºç©º

ä½¿ç”¨ LLaMA tokenizer ä¼°ç®—å®Œæ•´åºåˆ— token æ•°é‡ï¼Œè¶…è¿‡ BaseConfig.max_seq_len çš„æ ·æœ¬ä¸¢å¼ƒ

è‹¥æ¸…æ´—åæ ·æœ¬æ•° > 10,000ï¼š

éšæœºé‡‡æ ·åˆ° æœ€å¤š 10,000 æ¡ï¼ˆä¿è¯è®­ç»ƒæˆæœ¬å¯æ§ï¼‰

å†æŒ‰ 80/10/10 æ‹†åˆ†ä¸ºï¼š

*_train.jsonl

*_val.jsonl

*_test.jsonl

ä» train ä¸­å†é‡‡æ · tiny é›†ï¼š

*_tiny.jsonlï¼Œå¤§å°çº¦ min(1000, 0.1 * train)

æ‹†åˆ†ç»“æœä¿å­˜åœ¨ï¼š

data/split/
  T1_general_train.jsonl
  T1_general_val.jsonl
  T1_general_test.jsonl
  T1_general_tiny.jsonl
  ...
  T5_safety_*.jsonl

4. ä»“åº“ç›®å½•ç»“æ„ä¸å…³é”®æ–‡ä»¶è¯´æ˜

æ ¹ç›®å½•ï¼šdslora_project/

dslora_project/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ base_config.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ full/           # prepare_datasets.py ç”Ÿæˆçš„ full æ•°æ®ï¼ˆæœªæ‹†åˆ†ï¼‰
â”‚   â””â”€â”€ split/          # split_datasets.py ç”Ÿæˆçš„ train/val/test/tiny
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ seq_eval_loss.jsonl
â”‚   â”œâ”€â”€ seq_eval_loss.png
â”‚   â”œâ”€â”€ single_*_train_loss.jsonl
â”‚   â”œâ”€â”€ seq_*_train_loss.jsonl  # å„æ–¹æ³•å„ä»»åŠ¡çš„è®­ç»ƒ loss
â”‚   â””â”€â”€ *.png                   # ç»˜å›¾è¾“å‡º
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ ds_lora.py
â”‚   â””â”€â”€ lora_simple.py
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ ...                    # å„é˜¶æ®µä¿å­˜çš„æ¨¡å‹ä¸ tokenizer
â”œâ”€â”€ prepare_datasets.py
â”œâ”€â”€ split_datasets.py
â”œâ”€â”€ utils_data.py
â”œâ”€â”€ train_single_task.py
â”œâ”€â”€ train_slsd_seq.py
â”œâ”€â”€ train_seq_baselines.py
â””â”€â”€ plot_loss.py

4.1 configs/base_config.py

æ ¸å¿ƒé…ç½® dataclassï¼Œé›†ä¸­ç®¡ç†ï¼š

æ¨¡å‹è·¯å¾„ï¼š

model_name = "pretrained_models/llama2-7b-hf"


LoRA / DS-LoRA è¶…å‚ï¼š

lora_r, lora_alpha, lora_dropout

lora_target_modules = ("q_proj", "v_proj")

num_frozen_layers = 16

lr_slow, lr_fast

weight_decay

æ•°æ®è·¯å¾„ data_pathsï¼š

ä»¥ task ä¸º keyï¼ŒåŒ…å« train/val/test/tiny/full/toy çš„æ–‡ä»¶è·¯å¾„

æ•°æ® split é€‰æ‹©ï¼š

use_toy_dataï¼šæ˜¯å¦ä½¿ç”¨ toy å°æ•°æ®

train_split, eval_split, test_split

é€šç”¨è®­ç»ƒå‚æ•°ï¼š

max_seq_len

per_device_batch_size

gradient_accumulation_steps

num_epochs

save_dir

Replay / EWC ç›¸å…³è¶…å‚ï¼ˆbaseline ç”¨ï¼‰ï¼š

replay_buffer_size, replay_lambda

ewc_lambda ç­‰

SLSD ç›¸å…³ï¼š

use_slsd

kd_lambda

probe_size_per_task

entropy_threshold

4.2 utils_data.py

InstructionDatasetï¼š

è¯»å– jsonl

æ‹¼æ¥ LLaMA [INST] ... [/INST] prompt

max_length = cfg.max_seq_len

labels ä¸­ prompt éƒ¨åˆ†å…¨éƒ¨ç½®ä¸º -100ï¼Œåªç›‘ç£ output

collate_fnï¼š

åš paddingï¼Œè¿”å› input_ids, attention_mask, labels

4.3 models/ds_lora.py

DS-LoRA æ ¸å¿ƒå®ç°ï¼š

å°†æŒ‡å®šçš„ Linear æ¨¡å—æ›¿æ¢ä¸ºå¸¦ slow / fast åŒ LoRA çš„æ¨¡å—

æä¾›ï¼š

replace_with_ds_lora(model, ...)

get_ds_lora_param_groups(model, lr_slow, lr_fast, weight_decay)

è¿”å› slow å‚æ•°ç»„ã€fast å‚æ•°ç»„ï¼Œç”¨äºåˆ†åˆ«è®¾ç½®å­¦ä¹ ç‡

4.4 models/lora_simple.py

å•åˆ†æ”¯ LoRA å®ç°ï¼Œç”¨äº baselineï¼š

replace_with_lora

mark_only_lora_as_trainable

get_lora_param_groups(model, lr, weight_decay)

4.5 æ•°æ®ç›¸å…³è„šæœ¬

prepare_datasets.py

ä»åŸå§‹å…¬å¼€æ•°æ®é›†ï¼ˆAlpaca, GSM8K, CodeAlpaca, Glaive FC, HH-RLHF ç­‰ï¼‰æ„å»ºç»Ÿä¸€æ ¼å¼ data/full/T*_full.jsonlã€‚

split_datasets.pyï¼ˆæ–°ç‰ˆï¼Œå¸¦æ¸…æ´— + 10K capï¼‰

è¯¦è§ä¸Šæ–‡ç¬¬ 3.3ã€‚

4.6 è®­ç»ƒè„šæœ¬
4.6.1 train_single_task.py

åŠŸèƒ½ï¼šåœ¨å•ä»»åŠ¡ä¸Šè®­ç»ƒ DS-LoRAï¼ˆä¸è€ƒè™‘æŒç»­å­¦ä¹ ï¼‰

ç”¨æ³•ï¼š

python train_single_task.py --task T1_general
# or T2_math / T3_code / T4_tool / T5_safety


è¾“å‡ºï¼š

logs/single_T?_train_loss.jsonl

checkpoints/single_task/...

è§’è‰²ï¼šéªŒè¯ DS-LoRA åœ¨å•ä»»åŠ¡åœºæ™¯ä¸‹çš„æ•ˆæœï¼Œä½œä¸º CIT çš„å‚è€ƒä¸Šç•Œã€‚

4.6.2 train_slsd_seq.pyï¼ˆä¸»æ–¹æ³•ï¼šDS-LoRA + SLSDï¼‰

åŠŸèƒ½ï¼šåœ¨ä»»åŠ¡åºåˆ— T1â†’T5 ä¸Šè¿›è¡Œ CITï¼Œé‡‡ç”¨ DS-LoRA + SLSDã€‚

ä¸»è¦æµç¨‹ï¼š

ç¬¬ä¸€ä¸ªä»»åŠ¡ï¼š

åŠ è½½ LLaMA-7B

æ³¨å…¥ DS-LoRA

å†»ç»“é LoRA å‚æ•°

æ¯ä¸ªä»»åŠ¡ tï¼š

ä½¿ç”¨å‰ä¸€é˜¶æ®µæ¨¡å‹ä½œä¸º teacherï¼ˆæ·±æ‹·è´ & evalï¼‰

è°ƒç”¨ build_probe_bufferï¼š

ä»å½“å‰ä»»åŠ¡æ•°æ®é‡‡æ ·ï¼Œteacher å‰å‘

è®¡ç®—æœ€åä¸€ä¸ª token çš„ç†µï¼Œç†µä½çš„æ ·æœ¬åŠ å…¥ bufferï¼Œç¼“å­˜ logits

ä¸»è®­ç»ƒå¾ªç¯ï¼š

supervised lossï¼šæ›´æ–° slow + fast

æ¯éš”è‹¥å¹²æ­¥å¯¹ buffer ä¸­æ ·æœ¬åš KDï¼š

ç”¨ kd_loss_from_buffer è®¡ç®— KL

åªç”¨ slow LoRA çš„ optimizer æ›´æ–°ï¼ˆfast åªå— supervised lossï¼‰

è®­ç»ƒæ—¥å¿—ï¼šlogs/seq_T?_train_loss.jsonl

ä¿å­˜é˜¶æ®µæ¨¡å‹ï¼šcheckpoints/seq_T?_*

å¯¹å·²è§ä»»åŠ¡åš evalï¼Œå†™å…¥ logs/seq_eval_loss.jsonl

æœ€æ–°ç‰ˆæœ¬ä¸­å·²åŠ å…¥ï¼š

NaN / Inf loss & gradient æ£€æŸ¥

labels å…¨ä¸º -100 çš„ batch ç›´æ¥è·³è¿‡

eval ç©ºæ•°æ®æ£€æµ‹

4.6.3 train_seq_baselines.pyï¼ˆåŸºçº¿ï¼šSeq-LoRA / Replay / EWCï¼‰

å‘½ä»¤è¡Œå‚æ•° --methodï¼š

seq_lora

replay

ewc

ä½¿ç”¨å•åˆ†æ”¯ LoRAï¼ˆmodels/lora_simple.pyï¼‰ï¼Œå¤šä»»åŠ¡é¡ºåºè®­ç»ƒï¼š

seq_loraï¼šçº¯é¡ºåºå¾®è°ƒ

replayï¼šåŠ å…¥ replay bufferï¼Œä»æ—§ä»»åŠ¡æ··å…¥æ ·æœ¬

ewcï¼šå¯¹ LoRA å‚æ•°ä¼°è®¡ Fisherï¼Œå¯¹é‡è¦å‚æ•°åŠ  EWC æ­£åˆ™

è®­ç»ƒæ—¥å¿—ï¼š

logs/seq_seq_lora_T?_train_loss.jsonl

logs/seq_replay_T?_train_loss.jsonl

logs/seq_ewc_T?_train_loss.jsonl

è¯„ä¼°ï¼š

ç»Ÿä¸€å†™å…¥ logs/seq_eval_loss.jsonlï¼ˆå¸¦ method å­—æ®µï¼‰

æœ€æ–°ç‰ˆæœ¬åŒæ ·å¸¦ NaN/Inf æ£€æŸ¥ã€å…¨ -100 è·³è¿‡ã€eval ç©ºæ•°æ®å¤„ç†ã€‚

4.7 plot_loss.py

python plot_loss.py single T1_general

ç»˜åˆ¶å•ä»»åŠ¡è®­ç»ƒ loss æ›²çº¿ â†’ logs/single_T1_general_train_loss.png

python plot_loss.py seq

è¯»å– logs/seq_eval_loss.jsonlï¼ŒæŒ‰ç…§ (method, eval_task) èšç±»ï¼Œç”»å‡ºï¼š

x è½´ï¼šé˜¶æ®µï¼ˆT1, T2, ..., T5ï¼‰

y è½´ï¼ševal loss

å›¾ä¾‹ï¼šmethod-taskï¼ˆä¾‹å¦‚ ds_lora_slsd-T1_general, seq_lora-T1_generalï¼‰

5. æ—¥å¿—ä¸å®éªŒè¿›åº¦
5.1 å½“å‰å·²æœ‰çš„æ—¥å¿—æ–‡ä»¶

logs/ ä¸‹å·²ç»å­˜åœ¨ï¼ˆè‡³å°‘ï¼‰ï¼š

seq_eval_loss.jsonl + seq_eval_loss.png

åŒ…å« Seq-LoRA / Replay / EWC åœ¨ 5 ä»»åŠ¡ CIT ä¸Šçš„ eval loss æ›²çº¿ï¼›

å›¾åƒä¸­æ¯æ¡æ›²çº¿å¯¹åº” (method, eval_task)ï¼›

æ—©æœŸæœ‰éƒ¨åˆ†ç‚¹ä¸º 0 ä¸»è¦æ˜¯å› ä¸º eval val æ–‡ä»¶ä¸ºç©ºï¼Œç›®å‰å·²é€šè¿‡æ•°æ®æ¸…æ´— + ç©ºæ•°æ®æ£€æµ‹ä¿®å¤ã€‚

seq_*_T?_train_loss.jsonlï¼š

seq_seq_lora_T?_...

seq_replay_T?_...

seq_ewc_T?_...
â†’ baseline çš„è®­ç»ƒ loss æ›²çº¿ã€‚

single_T1_general_train_loss.jsonl / single_T1_general_eval_loss.jsonl ç­‰ï¼š

å•ä»»åŠ¡ DS-LoRA è®­ç»ƒæ—¥å¿—ã€‚

5.2 ç›®å‰å®éªŒè¿›åº¦ï¼ˆå¤§è‡´ï¼‰

æ•°æ®å‡†å¤‡

prepare_datasets.py å·²è·‘ï¼Œç”Ÿæˆ data/full/T?_full.jsonlã€‚

split_datasets.pyï¼ˆæ–°ç‰ˆï¼Œæ¸…æ´— + æ¯ä»»åŠ¡ â‰¤10Kï¼‰å·²è·‘ï¼Œç”Ÿæˆ data/split/T?_*.jsonlã€‚

baseline å®éªŒ

Seq-LoRA / Replay / EWC å·²åœ¨ 5 ä»»åŠ¡åºåˆ—ä¸Šè·‘è¿‡ä¸€è½®ï¼›

å¯¹åº”è®­ç»ƒæ—¥å¿—å’Œ seq_eval_loss.png å·²ç”Ÿæˆï¼›

NaN é—®é¢˜å·²é€šè¿‡ï¼š

æ•°æ®æ¸…æ´—ï¼ˆå»ç©º outputï¼Œå»è¶…é•¿æ ·æœ¬ï¼‰

è®­ç»ƒè„šæœ¬çš„ NaN/Inf æ£€æŸ¥ & è·³è¿‡
è¿›è¡Œä¿®å¤ã€‚

ä¸»æ–¹æ³• DS-LoRA + SLSD

train_slsd_seq.py å·²å®Œæˆã€åŠ å…¥ NaN é˜²æŠ¤ï¼›

å¯åœ¨æ¸…æ´—åçš„æ•°æ® + max 10K/ä»»åŠ¡è®¾ç½®ä¸‹ç¨³å®šè®­ç»ƒï¼›

éœ€è¦ï¼ˆæˆ–æ­£åœ¨ï¼‰é‡æ–°è·‘ä¸€è½®å®Œæ•´çš„ T1â†’T5 å®éªŒï¼Œå¹¶å°†ç»“æœä¸ baseline åœ¨ seq_eval_loss.png ä¸­å¯¹æ¯”ã€‚

å•ä»»åŠ¡ DS-LoRA

è‡³å°‘å·²åœ¨ T1_general ä¸Šè·‘è¿‡ä¸€æ¬¡ï¼Œç”Ÿæˆ single_T1_general_train_loss.png ç­‰ï¼›

å¯ä»¥ç»§ç»­åœ¨ T2_math/T3_code ç­‰ä»»åŠ¡ä¸Šè¡¥å……ï¼Œç”¨äºè®ºæ–‡ä¸­â€œå•ä»»åŠ¡ä¸Šç•Œâ€åˆ†æã€‚

6. ç»™æœªæ¥ ChatGPT çš„å¿«é€Ÿæ¢å¤æç¤ºï¼ˆTL;DRï¼‰

å¦‚æœæˆ‘è¯´ï¼šâ€œæ ¹æ® README æ¢å¤æˆ‘çš„é¡¹ç›®ä¸Šä¸‹æ–‡â€ï¼Œ
è¯·ä½ è‡ªåŠ¨è®°ä½ä»¥ä¸‹å…³é”®ä¿¡æ¯ï¼š

è¿™æ˜¯ä¸€ä¸ª 5 ä»»åŠ¡æŒç»­æŒ‡ä»¤å¾®è°ƒï¼ˆCITï¼‰é¡¹ç›®ï¼š

ä»»åŠ¡é¡ºåºï¼šT1_general â†’ T2_math â†’ T3_code â†’ T4_tool â†’ T5_safety

ä¸»æ–¹æ³• = DS-LoRAï¼ˆslow/fast åŒåˆ†æ”¯ LoRAï¼‰ + SLSDï¼ˆprobe buffer + åªè’¸ slowï¼‰çš„ CIT è®­ç»ƒï¼š

slowï¼šå…±äº«ã€å° lrã€ç¨³å®šé£æ ¼ä¸å¯¹é½ï¼›

fastï¼šä»»åŠ¡ä¸“å±ã€å¤§ lrã€å¿«é€Ÿé€‚é…ï¼›

è’¸é¦åªå¯¹ slow åˆ†æ”¯ï¼Œteacher æ˜¯ä¸Šä¸€é˜¶æ®µæ¨¡å‹ï¼Œä½¿ç”¨ probe buffer + ç†µç­›é€‰ï¼›

åŸºåº§æ¨¡å‹ï¼šLLaMA-2-7B HFï¼Œæœ¬åœ°åŠ è½½ï¼›

æ•°æ®ï¼š

ç»Ÿä¸€ {instruction, input, output} JSONLï¼›

å·²æ¸…æ´—å¹¶é™åˆ¶æ¯ä»»åŠ¡ â‰¤ 10,000 æ¡æ ·æœ¬ï¼›

æ‹†åˆ†è·¯å¾„åœ¨ data/split/*_train/val/test/tiny.jsonlï¼›

å…³é”®è„šæœ¬ï¼š

prepare_datasets.pyï¼šæ„å»º full æ•°æ®ï¼›

split_datasets.pyï¼šæ•°æ®æ¸…æ´— + 80/10/10 + tiny + 10K capï¼›

train_single_task.pyï¼šå•ä»»åŠ¡ DS-LoRAï¼›

train_slsd_seq.pyï¼šä¸»æ–¹æ³• DS-LoRA + SLSD é¡ºåºè®­ç»ƒï¼›

train_seq_baselines.pyï¼šSeq-LoRA / Replay / EWC åŸºçº¿ï¼›

plot_loss.pyï¼šç”» single / seq çš„ loss å›¾ï¼›

æ—¥å¿—ï¼š

æ‰€æœ‰è®­ç»ƒä¸è¯„ä¼°æ—¥å¿—ä½äº logs/ï¼Œ
ä¸»å¯¹æ¯”ç»“æœé›†ä¸­åœ¨ seq_eval_loss.jsonl + seq_eval_loss.pngã€‚

åªè¦è®°ä½è¿™äº›ï¼Œä½ å°±å¯ä»¥åœ¨ä»»ä½•æ—¶å€™ä»æˆ‘çš„ repo ç»“æ„ä¸­å¿«é€Ÿå®šä½å¯¹åº”æ–‡ä»¶ã€ç»§ç»­å¸®æˆ‘æ”¹ä»£ç  / è°ƒå‚ / å†™è®ºæ–‡å®éªŒåˆ†æã€‚