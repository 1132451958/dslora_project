# DS-LoRA & SLSD: Continual Instruction Tuning Framework

æœ¬é¡¹ç›®å®ç°äº†ç¡•å£«è®ºæ–‡æå‡ºçš„ **Dual-Speed LoRAï¼ˆDS-LoRAï¼‰** ä¸  
**Stability-aware Lightweight Self-Distillationï¼ˆSLSDï¼‰**ï¼Œç”¨äºè§£å†³æŒç»­æŒ‡ä»¤å¾®è°ƒ
ï¼ˆContinual Instruction Tuning, CITï¼‰ä¸­çš„ï¼š

- **è™šå‡é—å¿˜ (Illusory Forgetting)**
- **çŸ¥è¯†é—å¿˜ (Forgetting)**

è¯¥ä»“åº“åŒ…å«å®Œæ•´çš„å·¥ç¨‹å®ç°ã€è®­ç»ƒè„šæœ¬ã€æ•°æ®å‡†å¤‡æµç¨‹ä¸å¯è§†åŒ–å·¥å…·ã€‚

---

## ğŸš€ é¡¹ç›®äº®ç‚¹ï¼ˆè®ºæ–‡æ–¹æ³•ç®€ä»‹ï¼‰

### ğŸ”¹ æ–¹æ³•ä¸€ï¼šDual-Speed LoRAï¼ˆDS-LoRAï¼‰
ç”¨äºåœ¨æŒç»­è®­ç»ƒä¸­åŒæ—¶ä¿æŒ **ç¨³å®šæ€§ + å¯å¡‘æ€§**ã€‚

æ ¸å¿ƒæ€æƒ³ï¼š

1. **å†»ç»“åº•å±‚ Transformer å±‚**
   - ä¿æŒåŸºåº§æ¨¡å‹å¯¹é½æ€§ã€å®‰å…¨æ€§ã€è¯­è¨€é£æ ¼ç¨³å®š
   - é¿å…å‡é—å¿˜ï¼ˆé£æ ¼æ¼‚ç§»ï¼‰

2. **åœ¨é«˜å±‚ä¸ºæ¯ä¸ªçº¿æ€§å±‚æ³¨å…¥ Slow LoRA + Fast LoRA åŒåˆ†æ”¯**
   - Slow LoRAï¼šä»»åŠ¡å…±äº«ï¼Œå°å­¦ä¹ ç‡ â†’ ä¿æŒé•¿æœŸç¨³å®šè®°å¿†  
   - Fast LoRAï¼šä»»åŠ¡ç‹¬ç«‹ï¼Œå¤§å­¦ä¹ ç‡ â†’ å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡  
   - æœ€ç»ˆæƒé‡ï¼š  
     \[
     W = W_0 + \Delta W_{\text{slow}} + \Delta W_{\text{fast}}
     \]

3. **è®­ç»ƒç­–ç•¥**
   - Slow åˆ†æ”¯ï¼šå° lrï¼ˆä¾‹å¦‚ `1e-5`ï¼‰
   - Fast åˆ†æ”¯ï¼šå¤§ lrï¼ˆä¾‹å¦‚ `5e-5`ï¼‰
   - LoRA rank = 8ï¼Œdropout = 0.05

DS-LoRA è§£å†³ï¼š

- ğŸ¯ ä¸ç ´åæ—§ä»»åŠ¡æ€§èƒ½  
- ğŸš€ å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡  
- ğŸ’¡ é¿å…è™šå‡é—å¿˜ï¼ˆstyle shiftï¼‰

---

### ğŸ”¹ æ–¹æ³•äºŒï¼šSLSDï¼ˆè½»é‡çº§ç¨³å®šæ€§è‡ªè’¸é¦ï¼‰
è¿›ä¸€æ­¥ä¿è¯è®­ç»ƒè¿‡ç¨‹ä¸­ **è¾“å‡ºé£æ ¼ä¿æŒä¸€è‡´**ã€‚

æ ¸å¿ƒåšæ³•ï¼š

1. ä½¿ç”¨å½“å‰ä»»åŠ¡çš„å°‘é‡æ ·æœ¬ä½œä¸º probe bufferï¼ˆ100~500 æ¡ï¼‰
2. ç”¨ä¸Šä¸€é˜¶æ®µæ¨¡å‹ç”Ÿæˆ teacher logitsï¼ˆåªå‰å‘ä¸€æ¬¡ï¼‰
3. ç”¨è¾“å‡ºç†µç­›é€‰â€œä»£è¡¨æ—§é£æ ¼çš„éš¾å¿˜æ ·æœ¬â€
4. **è’¸é¦ä»…ä½œç”¨äº Slow LoRA åˆ†æ”¯**  
   - Slow LoRAï¼šsupervised loss + KD  
   - Fast LoRAï¼šä»… supervised loss  

SLSD åœ¨ä¿è¯è®­ç»ƒæ•ˆç‡çš„åŒæ—¶æ˜¾è‘—å‡å°‘é£æ ¼æ¼‚ç§»ã€‚

---

## ğŸ“‚ é¡¹ç›®ç»“æ„
dslora_project/
â”‚
â”œâ”€â”€ configs/
â”‚ â””â”€â”€ base_config.py # å…¨å±€é…ç½®ï¼ˆè·¯å¾„ã€LoRA å‚æ•°ã€lr ç­‰ï¼‰
â”‚
â”œâ”€â”€ data/ # äº”ä¸ª CIT ä»»åŠ¡æ•°æ®é›†ï¼ˆjsonlï¼‰
â”‚ â”œâ”€â”€ T1_general.jsonl
â”‚ â”œâ”€â”€ T2_math.jsonl
â”‚ â”œâ”€â”€ T3_code.jsonl
â”‚ â”œâ”€â”€ T4_tool.jsonl
â”‚ â””â”€â”€ T5_safety.jsonl
â”‚
â”œâ”€â”€ logs/ # è®­ç»ƒå¯è§†åŒ– logï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
â”‚
â”œâ”€â”€ models/
â”‚ â””â”€â”€ ds_lora.py # DS-LoRA æ ¸å¿ƒå®ç°
â”‚
â”œâ”€â”€ prepare_datasets.py # ä¸‹è½½å¹¶é¢„å¤„ç†äº”ä¸ªä»»åŠ¡æ•°æ®é›†
â”œâ”€â”€ utils_data.py # æ•°æ®å¤„ç†ã€collateã€tokenizer ç­‰
â”‚
â”œâ”€â”€ train_single_task.py # å•ä»»åŠ¡è®­ç»ƒï¼ˆæµ‹è¯• DS-LoRAï¼‰
â”œâ”€â”€ train_slsd_seq.py # æŒç»­ä»»åŠ¡åºåˆ—è®­ç»ƒï¼ˆDS-LoRA + SLSDï¼‰
â”‚
â”œâ”€â”€ plot_loss.py # loss å¯è§†åŒ–è„šæœ¬
â”‚
â””â”€â”€ checkpoints/ # ä¿å­˜æ¨¡å‹ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰


## âš™ï¸ ç¯å¢ƒå®‰è£…

å»ºè®®ä½¿ç”¨ condaï¼š

```bash
conda create -n dslora python=3.10 -y
conda activate dslora
å®‰è£… PyTorchï¼ˆCUDA 11ï¼‰
bash
å¤åˆ¶ä»£ç 
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
å®‰è£…å…¶ä½™ä¾èµ–
bash
å¤åˆ¶ä»£ç 
pip install transformers datasets tqdm matplotlib accelerate
ğŸ§ª æ•°æ®é›†å‡†å¤‡
è¿è¡Œï¼š

bash
å¤åˆ¶ä»£ç 
python prepare_datasets.py
è„šæœ¬ä¼šè‡ªåŠ¨ç”Ÿæˆ 5 ä¸ª CIT ä»»åŠ¡çš„æ•°æ®é›†ï¼š

é€šç”¨ä»»åŠ¡ï¼ˆT1ï¼‰

æ•°å­¦ï¼ˆT2ï¼‰

ä»£ç ï¼ˆT3ï¼‰

å·¥å…·è°ƒç”¨ï¼ˆT4ï¼‰

å®‰å…¨ï¼ˆT5ï¼‰

ğŸ”¥ è®­ç»ƒæµç¨‹
â–¶ 1. å•ä»»åŠ¡è®­ç»ƒï¼ˆéªŒè¯ DS-LoRAï¼‰
ç¤ºä¾‹ï¼š

bash
å¤åˆ¶ä»£ç 
python train_single_task.py --task T1_general
å…¶ä»–ä»»åŠ¡ï¼š

bash
å¤åˆ¶ä»£ç 
python train_single_task.py --task T2_math
python train_single_task.py --task T3_code
...
è¾“å‡ºåŒ…æ‹¬ï¼š

logs/single_*.jsonl

checkpoints/single_*

â–¶ 2. æŒç»­è®­ç»ƒï¼ˆDS-LoRA + SLSDï¼‰
bash
å¤åˆ¶ä»£ç 
python train_slsd_seq.py
è®­ç»ƒé¡ºåºï¼š

nginx
å¤åˆ¶ä»£ç 
T1 â†’ T2 â†’ T3 â†’ T4 â†’ T5
æ¯é˜¶æ®µè‡ªåŠ¨è¯„ä¼°å·²è§ä»»åŠ¡ï¼Œè®°å½•åˆ°ï¼š

bash
å¤åˆ¶ä»£ç 
logs/seq_eval_loss.jsonl
æ¨¡å‹ä¿å­˜è·¯å¾„ï¼š

bash
å¤åˆ¶ä»£ç 
checkpoints/seq_T1_general/
checkpoints/seq_T2_math/
...
ğŸ“ˆ å¯è§†åŒ–
å•ä»»åŠ¡ loss æ›²çº¿ï¼š
bash
å¤åˆ¶ä»£ç 
python plot_loss.py single T1_general
è¾“å‡ºæ–‡ä»¶ï¼š

bash
å¤åˆ¶ä»£ç 
logs/single_T1_general_train_loss.png
åºåˆ—ä»»åŠ¡ loss æ›²çº¿ï¼š
bash
å¤åˆ¶ä»£ç 
python plot_loss.py seq
è¾“å‡ºæ–‡ä»¶ï¼š

bash
å¤åˆ¶ä»£ç 
logs/seq_eval_loss.png
â— å¸¸è§é—®é¢˜
Loss å‡ºç° NaNï¼Ÿ
ä½¿ç”¨ float32ï¼ˆå·²é»˜è®¤ï¼‰

é™ä½å­¦ä¹ ç‡ï¼š

python
å¤åˆ¶ä»£ç 
lr_slow = 1e-5
lr_fast = 5e-5
LoRA dropout ä¿æŒ 0.05

å¢åŠ  NaN æ£€æŸ¥åŠæ—¶è·³è¿‡åæ ·æœ¬

CUDA OOMï¼Ÿ
é™ä½ batch size

æå‡ gradient_accumulation_steps

ç¼©çŸ­ max_seq_length

ğŸ™ è‡´è°¢
æœ¬é¡¹ç›®åŸºäºï¼š

HuggingFace Transformers

PyTorch

LoRAï¼ˆHu et al.ï¼‰

ä½ çš„ç¡•å£«æ¯•ä¸šè®ºæ–‡æ‰€æå‡ºçš„æ–¹æ³•

æ¬¢è¿å¼•ç”¨ã€æ‰©å±•ä¸æ”¹è¿›ã€‚
