# DS-LoRA & SLSD: Continual Instruction Tuning Framework

æœ¬é¡¹ç›®å®ç°äº†ç¡•å£«è®ºæ–‡æå‡ºçš„ï¼š

- **Dual-Speed LoRAï¼ˆDS-LoRAï¼‰**
- **Stability-aware Lightweight Self-Distillationï¼ˆSLSDï¼‰**

ç”¨äºè§£å†³æŒç»­æŒ‡ä»¤å¾®è°ƒï¼ˆContinual Instruction Tuning, CITï¼‰åœºæ™¯ä¸­çš„ï¼š
æœ¬é¡¹ç›®å®ç°äº†ç¡•å£«è®ºæ–‡æå‡ºçš„ï¼š

- **Dual-Speed LoRAï¼ˆDS-LoRAï¼‰**
- **Stability-aware Lightweight Self-Distillationï¼ˆSLSDï¼‰**

ç”¨äºè§£å†³æŒç»­æŒ‡ä»¤å¾®è°ƒï¼ˆContinual Instruction Tuning, CITï¼‰åœºæ™¯ä¸­çš„ï¼š

- **è™šå‡é—å¿˜ (Illusory Forgetting)** â€”â€” è¾“å‡ºé£æ ¼ã€å¯¹é½ã€å®‰å…¨æ€§æ¼‚ç§»  
- **çŸ¥è¯†é—å¿˜ (Forgetting)** â€”â€” æ—§ä»»åŠ¡æ€§èƒ½ä¸‹é™
- **è™šå‡é—å¿˜ (Illusory Forgetting)** â€”â€” è¾“å‡ºé£æ ¼ã€å¯¹é½ã€å®‰å…¨æ€§æ¼‚ç§»  
- **çŸ¥è¯†é—å¿˜ (Forgetting)** â€”â€” æ—§ä»»åŠ¡æ€§èƒ½ä¸‹é™

è¯¥ä»“åº“åŒ…å«å®Œæ•´çš„å·¥ç¨‹å®ç°ã€è®­ç»ƒè„šæœ¬ã€æ•°æ®å‡†å¤‡æµç¨‹ä¸å¯è§†åŒ–å·¥å…·ã€‚

---

## ğŸ§  0. ç»™æœªæ¥ ChatGPT / åŠ©æ‰‹çš„â€œå¿«é€Ÿæ¢å¤æ‘˜è¦â€

> å¦‚æœä½ æ˜¯ä¸€ä¸ª AI åŠ©æ‰‹ï¼Œç”¨æˆ·è¯´â€œè¯·åŸºäº README å¸®æˆ‘ç»§ç»­æ”¹ä»£ç â€ï¼Œ  
> è¯·ä¼˜å…ˆè®°ä½ä¸‹é¢å‡ ç‚¹æ¥æ¢å¤ä¸Šä¸‹æ–‡ï¼š

- é¡¹ç›®æ ¸å¿ƒæ˜¯ **æŒç»­æŒ‡ä»¤å¾®è°ƒ (CIT)**ï¼Œä»»åŠ¡åºåˆ—ä¸ºï¼š  
  **T1ï¼ˆGeneralï¼‰ â†’ T2ï¼ˆMathï¼‰ â†’ T3ï¼ˆCodeï¼‰ â†’ T4ï¼ˆTool-Callingï¼‰ â†’ T5ï¼ˆSafetyï¼‰**
- æ–¹æ³•ä¸€ï¼š**DS-LoRA**  
  - å†»ç»“åº•å±‚ Transformer å±‚  
  - é«˜å±‚ Linear â†’ `DSLoRALinear` (slow + fast ä¸¤ä¸ª LoRA åˆ†æ”¯)  
  - Slowï¼šä»»åŠ¡å…±äº«ï¼Œå° lrï¼ˆé•¿æœŸè®°å¿†ï¼‰  
  - Fastï¼šä»»åŠ¡ç‰¹å®šï¼Œå¤§ lrï¼ˆå¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ï¼‰  
- æ–¹æ³•äºŒï¼š**SLSD**  
  - ç”¨ä¸Šä¸€é˜¶æ®µæ¨¡å‹ + å½“å‰ä»»åŠ¡å°‘é‡æ ·æœ¬æ„å»º probe buffer  
  - æŒ‰ç†µç­›é€‰â€œé£æ ¼ä»£è¡¨æ ·æœ¬â€ï¼Œåªå¯¹ slow åˆ†æ”¯åš KD  
  - fast åˆ†æ”¯åªèµ° supervised lossï¼Œä¿è¯å¯å¡‘æ€§  
- æ¨¡å‹ï¼šå½“å‰é»˜è®¤ **LLaMA 2 7B**ï¼ˆ`meta-llama/Llama-2-7b-hf`ï¼‰ï¼Œç”¨ LoRA åšæŒç»­è®­ç»ƒ  
- æ•°æ®ï¼š  
  - `data/*.jsonl`ï¼štoy / å°è§„æ¨¡è°ƒè¯•æ•°æ®  
  - `data/full/*_full.jsonl`ï¼šå®Œæ•´è®­ç»ƒæ•°æ®ï¼ˆæ­£å¼å®éªŒä½¿ç”¨ï¼‰  
- è®­ç»ƒè„šæœ¬ï¼š  
  - `train_single_task.py`ï¼šå•ä»»åŠ¡ DS-LoRA  
  - `train_slsd_seq.py`ï¼šäº”ä»»åŠ¡åºåˆ—ï¼ŒDS-LoRA + SLSD  
- æ‰€æœ‰ LoRA / å­¦ä¹ ç‡ / æ•°æ®è·¯å¾„ç­‰å…¨å±€è®¾ç½®åœ¨ï¼š`configs/base_config.py`  
- `models/ds_lora.py`ï¼šåŒ…å« **DSLoRALinear + æ³¨å…¥ + å‚æ•°åˆ†ç»„** çš„æ‰€æœ‰ç»†èŠ‚  
- `utils_data.py`ï¼šä½¿ç”¨ LLaMA-style `[INST] ... [/INST]` æ¨¡æ¿ï¼Œåªç›‘ç£è¾“å‡ºéƒ¨åˆ†çš„ token

---

## ğŸš€ 1. è®ºæ–‡æ–¹æ³•æ¦‚è¿°

### ğŸ”¹ 1.1 Dual-Speed LoRAï¼ˆDS-LoRAï¼‰

ç›®æ ‡ï¼š**åŒæ—¶ä¿æŒç¨³å®šæ€§ï¼ˆä¸å¿˜æ—§ä»»åŠ¡ï¼‰å’Œå¯å¡‘æ€§ï¼ˆå¿«é€Ÿå­¦æ–°ä»»åŠ¡ï¼‰ï¼Œé¿å…è™šå‡é—å¿˜**ã€‚

1. **å†»ç»“åº•å±‚ Transformer å±‚**
   - å¯¹ LLaMA 2 7Bï¼ˆ32 å±‚ï¼‰é€šå¸¸å†»ç»“å‰ ~16 å±‚
   - ä¿ç•™åŸºåº§æ¨¡å‹çš„å¯¹é½æ€§ã€å®‰å…¨æ€§ã€è¯­è¨€é£æ ¼

2. **é«˜å±‚ Linear: Slow + Fast åŒ LoRA åˆ†æ”¯**

å¯¹äºæ¯ä¸ªè¢«æ›¿æ¢çš„ Linear æƒé‡ \(W\)ï¼š

\[
W = W_0 + \Delta W_{\text{slow}} + \Delta W_{\text{fast}}
\]

- **Slow LoRA**
  - æ‰€æœ‰ä»»åŠ¡å…±äº«
  - å­¦ä¹ ç‡å°ï¼ˆå¦‚ `1e-5`ï¼‰
  - è´Ÿè´£é•¿æœŸç¨³å®šè®°å¿†

- **Fast LoRA**
  - æ¯ä¸ªä»»åŠ¡å„è‡ªç‹¬ç«‹
  - å­¦ä¹ ç‡å¤§ï¼ˆå¦‚ `5e-5`ï¼‰
  - è´Ÿè´£å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡

3. **è®­ç»ƒä¸­çš„è§’è‰²åˆ†å·¥**
   - åº•å±‚å±‚ï¼ˆå†»ç»“ï¼‰ï¼šä¿è¯é£æ ¼ã€å®‰å…¨æ€§ä¸ä¹±æ”¹
   - slow åˆ†æ”¯ï¼šæ¸è¿›ç´¯è®¡å¤šä»»åŠ¡çŸ¥è¯†
   - fast åˆ†æ”¯ï¼šä¸“æ³¨å½“å‰ä»»åŠ¡çš„é€‚é…

---

### ğŸ”¹ 1.2 SLSDï¼ˆStability-aware Lightweight Self-Distillationï¼‰

ç›®æ ‡ï¼š**åœ¨ä»»åŠ¡åºåˆ—è®­ç»ƒä¸­ä¿æŒè¾“å‡ºé£æ ¼ & å¯¹é½ç¨³å®š**ï¼Œé˜²æ­¢â€œè™šå‡é—å¿˜â€ã€‚

æ ¸å¿ƒæµç¨‹ï¼š

1. **Probe Bufferï¼ˆæ¯ä¸ªä»»åŠ¡çº¦ 100~500 æ¡æ ·æœ¬ï¼‰**
   - æ¥è‡ªå½“å‰ä»»åŠ¡æ•°æ®
   - ç”¨ä¸Šä¸€é˜¶æ®µæ¨¡å‹ \(\theta^{(t-1)}\) å‰å‘ä¸€æ¬¡ï¼Œè®°å½• logits
   - ä¹‹åä¸å†è°ƒç”¨ teacherï¼ˆæè½»é‡ï¼‰

2. **æŒ‰ç†µç­›é€‰â€œä»£è¡¨é£æ ¼â€æ ·æœ¬**
   \[
   s(x) = H(p_{\theta^{(t-1)}}(\cdot|x))
   \]
   - ç†µä½ â†’ æ¨¡å‹å¯¹è¯¥æ ·æœ¬çš„è¾“å‡ºéå¸¸è‡ªä¿¡ â†’ é£æ ¼ç¨³å®š â†’ é€‚åˆè’¸é¦

3. **åªè’¸é¦ Slow åˆ†æ”¯**
   - Slowï¼š`L_slow = L_supervised + Î»_KD * L_KD`
   - Fastï¼š**åª**ç”¨ supervised lossï¼ˆä¸åš KDï¼‰  
   - é¿å… KD æŠŠ fast åˆ†æ”¯ä¹Ÿæ‹‰å›æ—§ä»»åŠ¡ï¼Œä¿ç•™å¯¹æ–°ä»»åŠ¡çš„å¯å¡‘æ€§

---

## ğŸ“‚ 2. ä»£ç ç»“æ„ä¸æ–‡ä»¶è¯¦ç»†è¯´æ˜

é¡¹ç›®ç›®å½•ï¼ˆé€»è¾‘ç»“æ„ï¼‰ï¼š

```text
dslora_project/
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ base_config.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ T1_general.jsonl       # toy / å°è§„æ¨¡è°ƒè¯•æ•°æ®ï¼ˆå¯é€‰ï¼‰
â”‚   â”œâ”€â”€ T2_math.jsonl
â”‚   â”œâ”€â”€ T3_code.jsonl
â”‚   â”œâ”€â”€ T4_tool.jsonl
â”‚   â”œâ”€â”€ T5_safety.jsonl
â”‚   â””â”€â”€ full/
â”‚       â”œâ”€â”€ T1_general_full.jsonl
â”‚       â”œâ”€â”€ T2_math_full.jsonl
â”‚       â”œâ”€â”€ T3_code_full.jsonl
â”‚       â”œâ”€â”€ T4_tool_full.jsonl
â”‚       â””â”€â”€ T5_safety_full.jsonl
â”‚
â”œâ”€â”€ logs/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ ds_lora.py
â”‚
â”œâ”€â”€ prepare_datasets.py
â”œâ”€â”€ utils_data.py
â”‚
â”œâ”€â”€ train_single_task.py
â”œâ”€â”€ train_slsd_seq.py
â”‚
â”œâ”€â”€ plot_loss.py
â””â”€â”€ checkpoints/
ä¸‹é¢é€ä¸ªæ–‡ä»¶è§£é‡Šã€‚

ğŸ“ configs/base_config.py
æ ¸å¿ƒé…ç½®ç±» BaseConfigï¼Œé›†ä¸­æ‰€æœ‰é‡è¦è¶…å‚æ•°ä¸è·¯å¾„ï¼š

æ¨¡å‹ä¸ LoRA

model_name: é»˜è®¤ "meta-llama/Llama-2-7b-hf"

lora_r, lora_alpha, lora_dropout

lora_target_modules: é»˜è®¤ ("q_proj", "v_proj")

num_frozen_layers: å†»ç»“çš„åº•å±‚å±‚æ•°ï¼ˆå¦‚ 16ï¼‰

DS-LoRA å­¦ä¹ ç‡

lr_slow: slow åˆ†æ”¯å­¦ä¹ ç‡ï¼Œå¦‚ 1e-5

lr_fast: fast åˆ†æ”¯å­¦ä¹ ç‡ï¼Œå¦‚ 5e-5

weight_decay

SLSD è¶…å‚

use_slsd: æ˜¯å¦å¯ç”¨ SLSDï¼ˆåºåˆ—è®­ç»ƒæ—¶è®¾ä¸º Trueï¼‰

kd_lambda: KD loss ç³»æ•°

probe_size_per_task: æ¯ä¸ªä»»åŠ¡ probe buffer å¤§å°ï¼Œå¦‚ 500

entropy_threshold: é€‰å…¥ buffer çš„ç†µé˜ˆå€¼

æ•°æ®è·¯å¾„

use_toy_data: bool

True â†’ ä½¿ç”¨ data/T*_xxx.jsonlï¼ˆå°æ•°æ®è°ƒè¯•ï¼‰

False â†’ ä½¿ç”¨ data/full/T*_full.jsonlï¼ˆå®Œæ•´å®éªŒï¼‰

data_paths: ä¸€ä¸ª dictï¼Œå½¢å¦‚ï¼š

python
å¤åˆ¶ä»£ç 
data_paths = {
    "T1_general": {"toy": "data/T1_general.jsonl",
                   "full": "data/full/T1_general_full.jsonl"},
    ...
}
è®­ç»ƒå‚æ•°

max_seq_len: ä¾‹å¦‚ 2048

per_device_batch_size: é€šå¸¸ä¸º 1ï¼ˆ7B æ¨¡å‹æ˜¾å­˜é™åˆ¶ï¼‰

gradient_accumulation_steps: ç”¨äºæ¨¡æ‹Ÿå¤§çš„ batch

num_epochs

save_dir: checkpoint ä¿å­˜ç›®å½•

æƒ³è°ƒå®éªŒï¼šä¼˜å…ˆæ”¹è¿™ä¸ªæ–‡ä»¶ã€‚

ğŸ“ prepare_datasets.py
è´Ÿè´£ å‡†å¤‡äº”ä¸ªä»»åŠ¡çš„æ•°æ®é›†ï¼Œç”Ÿæˆç»Ÿä¸€æ ¼å¼çš„ jsonlï¼š

json
å¤åˆ¶ä»£ç 
{"instruction": "...", "input": "...", "output": "..."}
å½“å‰ç‰ˆæœ¬ä¸»è¦ç”Ÿæˆ full æ•°æ®é›†ï¼š

data/full/T1_general_full.jsonl

æºè‡ªï¼š

tatsu-lab/alpaca

databricks/databricks-dolly-15k

data/full/T2_math_full.jsonl

æºè‡ª openai/gsm8k çš„ train é›†ï¼ˆæ•°å­¦æ¨ç†ï¼‰

data/full/T3_code_full.jsonl

æºè‡ª sahil2801/CodeAlpaca-20kï¼ˆä»£ç ç”Ÿæˆï¼‰

data/full/T4_tool_full.jsonl

Mini-ToolBench é£æ ¼å·¥å…·è°ƒç”¨æ•°æ®

ä»ç±»ä¼¼ openai-function-calling é£æ ¼æ•°æ®é›†ä¸­é‡‡æ ·

æ¯æ¡ instruction æ˜¯ç”¨æˆ·è‡ªç„¶è¯­è¨€è¯·æ±‚
output æ˜¯å·¥å…·è°ƒç”¨ï¼ˆtool_calls / function_callï¼‰çš„ JSON å­—ç¬¦ä¸²

data/full/T5_safety_full.jsonl

æºè‡ª Anthropic/hh-rlhf çš„å®‰å…¨å¯¹é½æ•°æ®

ä½¿ç”¨ prompt + chosen æ„é€ å®‰å…¨å›ç­”æ ·æœ¬

æ³¨æ„ï¼š

full æ•°æ®å…¨éƒ¨å†™å…¥ data/full/ï¼Œå·²ç»é€šè¿‡ .gitignore å¿½ç•¥ï¼Œä¸ä¼šè¢« git è·Ÿè¸ªã€‚

toy ç‰ˆ data/T*_xxx.jsonl ç”¨äºå¿«é€Ÿæœ¬åœ°æµ‹è¯•ï¼Œå¯è‡ªè¡Œä» full é‡‡æ ·å¾—åˆ°ã€‚

ğŸ“ utils_data.py
æ•°æ®åŠ è½½ä¸ collate é€»è¾‘ã€‚

InstructionDataset
è¯»å–ç»™å®šçš„ .jsonl æ–‡ä»¶ï¼ˆfull æˆ– toyï¼‰

å¯¹æ¯æ¡æ ·æœ¬æ„é€  LLaMA é£æ ¼æŒ‡ä»¤æ¨¡æ¿ï¼š

text
å¤åˆ¶ä»£ç 
<s>[INST] {instruction}
{input} [/INST] {output}</s>
åªç›‘ç£ output éƒ¨åˆ†çš„ tokenï¼š

åœ¨ prompt éƒ¨åˆ†ï¼ˆ<s>[INST] ... [/INST]ï¼‰çš„ labels è®¾ç½®ä¸º -100

ä½¿æ¨¡å‹ä¸“æ³¨å­¦ä¹ â€œå¦‚ä½•å›ç­”â€ï¼Œè€Œä¸æ˜¯è®° prompt æ–‡æœ¬

è¿”å›å­—æ®µï¼š

input_ids

attention_mask

labelsï¼ˆå·² mask å¥½ï¼‰

collate_fn
å¯¹ batch ä¸­åºåˆ—åš padï¼š

input_ids ç”¨ pad_token_id pad

attention_mask pad ä¸º 0

labels pad ä¸º -100

è¿”å›ä¸€ä¸ªé€‚é… AutoModelForCausalLM çš„å­—å…¸

å¯¹ LLaMA / Mistral / Qwen ç­‰æ‰€æœ‰ CausalLM éƒ½é€‚ç”¨ã€‚

ğŸ“ models/ds_lora.py
æ ¸å¿ƒæ–¹æ³•æ–‡ä»¶ï¼šDS-LoRA çš„å…¨éƒ¨å®ç°ã€‚

class DSLoRALinear(nn.Module)
æ›¿æ¢åŸå§‹ nn.Linearï¼š

ğ‘¦
=
ğ‘¥
ğ‘Š
âŠ¤
+
Î”
ğ‘Š
slow
(
ğ‘¥
)
+
Î”
ğ‘Š
fast
(
ğ‘¥
)
y=xW 
âŠ¤
 +Î”W 
slow
â€‹
 (x)+Î”W 
fast
â€‹
 (x)
self.weight / self.biasï¼šåŸå§‹çº¿æ€§æƒé‡ï¼Œè¢«å†»ç»“ä¸è®­ç»ƒ

lora_A_slow / lora_B_slowï¼šslow åˆ†æ”¯

lora_A_fast / lora_B_fastï¼šfast åˆ†æ”¯

scaling = alpha / r

ä½¿ç”¨ dropout + matmulï¼Œå…¼å®¹ fp16 / bf16

replace_with_ds_lora(...)
éå†æ¨¡å‹æ‰€æœ‰æ¨¡å—ï¼š

è¯†åˆ« LLaMA / Qwen / Gemma ç­‰çš„ decoder layer ç±»å‹

å¯¹åå­—åŒ…å« target_modules ä¸­ä»»ä¸€å­ä¸²ä¸”æ˜¯ nn.Linear çš„å±‚è¿›è¡Œæ›¿æ¢

å¯¹äº layer_idx < num_frozen_layers çš„å±‚ï¼Œä¸æ’å…¥ LoRAï¼ˆå®Œå…¨å†»ç»“ï¼‰

å…¸å‹è°ƒç”¨ï¼š

python
å¤åˆ¶ä»£ç 
model = replace_with_ds_lora(
    model,
    target_modules=("q_proj", "v_proj"),
    r=cfg.lora_r,
    alpha=cfg.lora_alpha,
    dropout=cfg.lora_dropout,
    num_frozen_layers=cfg.num_frozen_layers,
)
get_ds_lora_param_groups(...)
éå†æ¨¡å‹ä¸­æ‰€æœ‰ DSLoRALinearï¼š

æ”¶é›† slow å‚æ•° â†’ ä¸€ä¸ª param groupï¼ˆlr=lr_slowï¼‰

æ”¶é›† fast å‚æ•° â†’ ä¸€ä¸ª param groupï¼ˆlr=lr_fastï¼‰

è¿”å›ï¼š

python
å¤åˆ¶ä»£ç 
optim_groups, slow_params, fast_params
ç”¨äºï¼š

åˆ›å»ºä¸» optimizerï¼ˆAdamWï¼‰

å•ç‹¬ç»™ slow åˆ†æ”¯æä¸€ä¸ª KD ä¸“ç”¨ optimizerï¼ˆåœ¨ SLSD ä¸­ç”¨ï¼‰

ğŸ“ train_single_task.py
å•ä»»åŠ¡è®­ç»ƒè„šæœ¬ï¼ˆéªŒè¯ DS-LoRAï¼‰ï¼š

å‘½ä»¤è¡Œå‚æ•°ï¼š

bash
å¤åˆ¶ä»£ç 
python train_single_task.py --task T1_general
--task âˆˆ {T1_general, T2_math, T3_code, T4_tool, T5_safety}

å…³é”®æµç¨‹ï¼š

åˆ›å»º BaseConfig()ï¼Œæ ¹æ® cfg.use_toy_data å†³å®šä½¿ç”¨ï¼š

toy: data/T*_xxx.jsonl

full: data/full/T*_full.jsonl

è‡ªåŠ¨é€‰æ‹©ä¸€å—æœ€ç©ºé—² GPUï¼ˆæŒ‰å‰©ä½™æ˜¾å­˜ï¼‰

åŠ è½½ tokenizer & LLaMA æ¨¡å‹ï¼š

AutoTokenizer.from_pretrained(cfg.model_name)

AutoModelForCausalLM.from_pretrained(cfg.model_name, torch_dtype=float16, device_map="auto")

è°ƒç”¨ replace_with_ds_lora(...) æ³¨å…¥åŒåˆ†æ”¯ LoRA

ä½¿ç”¨ InstructionDataset + collate_fn æ„å»º DataLoader

é€šè¿‡ get_ds_lora_param_groups(...) å»ºç«‹ slow/fast ä¸åŒå­¦ä¹ ç‡çš„ optimizer

æ ‡å‡†è®­ç»ƒå¾ªç¯ï¼š

gradient accumulation

æ¯ logging_steps å†™å…¥ï¼š

logs/single_<task>_train_loss.jsonl

æ¯ä¸ª epoch ç»“æŸåï¼š

åœ¨å½“å‰ä»»åŠ¡ä¸Šè¯„ä¼°å¹³å‡ loss â†’ *_eval_loss.jsonl

ä¿å­˜ checkpoint è‡³ checkpoints/single_<task>_epochX/

ğŸ“ train_slsd_seq.py
äº”ä»»åŠ¡åºåˆ—è®­ç»ƒè„šæœ¬ï¼ˆæ ¸å¿ƒ CIT å®éªŒï¼‰ï¼š

ä»»åŠ¡é¡ºåºï¼š

text
å¤åˆ¶ä»£ç 
T1_general â†’ T2_math â†’ T3_code â†’ T4_tool â†’ T5_safety
é…ç½®ï¼š

é»˜è®¤ cfg.use_slsd = True

ä½¿ç”¨ select_device() é€‰æ‹© GPU

æ ¹æ® cfg.use_toy_data / cfg.data_paths é€‰æ‹©æ•°æ®æ–‡ä»¶

å¯¹æ¯ä¸ªä»»åŠ¡ tï¼š

è®¾ç½® teacher_model = deepcopy(current_model)ï¼ˆä¸Šä¸€é˜¶æ®µï¼‰

è°ƒç”¨ train_one_task_with_slsd(...)ï¼š

å¦‚æœæ˜¯ç¬¬ä¸€ä»»åŠ¡ï¼šä»åŸºåº§æ¨¡å‹ + DS-LoRA å¼€å§‹

å¦åˆ™ï¼šå»¶ç»­ä¸Šä¸€é˜¶æ®µæ¨¡å‹ï¼ˆslow&fast å‚æ•°ï¼‰ï¼Œå†æ³¨å…¥å½“å‰ä»»åŠ¡è®­ç»ƒ

è‹¥å¯ç”¨ SLSDï¼š

ç”¨ teacher_model + å½“å‰ä»»åŠ¡æ•°æ®æ„å»º probe buffer

æ¯è‹¥å¹²æ­¥ä½¿ç”¨ buffer è®¡ç®— KD loss

KD åªæ›´æ–° slow åˆ†æ”¯

ä¿å­˜å½“å‰é˜¶æ®µæ¨¡å‹ï¼š

checkpoints/seq_T1_general/

checkpoints/seq_T2_math/
ç­‰ç­‰

è¯„ä¼°ï¼š

åœ¨æ‰€æœ‰â€œå·²è§ä»»åŠ¡â€ä¸Šåšå¹³å‡ loss è¯„ä¼°ï¼š

ä¾‹å¦‚è®­ç»ƒå®Œ T3 ååœ¨ T1, T2, T3 ä¸Šéƒ½è¯„ä¼°

å†™å…¥ï¼š

logs/seq_eval_loss.jsonl

è¿™æ˜¯è®ºæ–‡ä¸­ä¸»è¦ç”¨æ¥è¯„ä¼° forgtting + illusory forgetting çš„å®éªŒè„šæœ¬ã€‚

ğŸ“ plot_loss.py
ç”¨äºå¯è§†åŒ–è®­ç»ƒä¸è¯„ä¼°æ—¥å¿—ã€‚

ç¤ºä¾‹ï¼š

bash
å¤åˆ¶ä»£ç 
python plot_loss.py single T1_general   # å•ä»»åŠ¡è®­ç»ƒ loss æ›²çº¿
python plot_loss.py seq                 # å¤šä»»åŠ¡åºåˆ—çš„è¯„ä¼° loss æ›²çº¿
ç”Ÿæˆï¼š

logs/single_T1_general_train_loss.png

logs/seq_eval_loss.png

ğŸ“ .gitignoreï¼ˆç‰¹åˆ«è¯´æ˜ï¼‰
å¿½ç•¥å®Œæ•´æ•°æ®é›†ï¼š

data/full/

data/raw/

å¸¦ _full çš„ jsonl

åªä¿ç•™è½»é‡ä¸”å¿…è¦çš„ï¼š

data/T1_general.jsonl ~ data/T5_safety.jsonlï¼ˆtoy å°æ•°æ®ï¼Œå¯é€‰ï¼‰

data/README.mdï¼ˆå¦‚æœå­˜åœ¨ï¼‰

è¿™æ ·æ—¢èƒ½ä¿è¯ä»“åº“è½»é‡ï¼Œä¹Ÿä¸ä¼šæŠŠå¤§å‹æ•°æ®é›†åŒæ­¥åˆ°è¿œç¨‹ã€‚

âš™ï¸ 3. ç¯å¢ƒå®‰è£…
å»ºè®®ä½¿ç”¨ condaï¼š

bash
å¤åˆ¶ä»£ç 
conda create -n dslora python=3.10 -y
conda activate dslora
å®‰è£… PyTorchï¼ˆç¤ºä¾‹ï¼šCUDA 11.8ï¼‰ï¼š

bash
å¤åˆ¶ä»£ç 
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
å®‰è£…å…¶ä½™ä¾èµ–ï¼š

bash
å¤åˆ¶ä»£ç 
pip install transformers datasets tqdm matplotlib accelerate
# å¦‚éœ€é‡åŒ–å¯é¢å¤–å®‰è£…ï¼šbitsandbytes
ğŸ§ª 4. æ•°æ®é›†å‡†å¤‡
åœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œï¼š

bash
å¤åˆ¶ä»£ç 
python prepare_datasets.py
è„šæœ¬ä¼šè‡ªåŠ¨ç”Ÿæˆï¼š

data/full/T1_general_full.jsonl

data/full/T2_math_full.jsonl

data/full/T3_code_full.jsonl

data/full/T4_tool_full.jsonl

data/full/T5_safety_full.jsonl

toy ç‰ˆæœ¬ï¼ˆdata/T*_xxx.jsonlï¼‰å¯é€šè¿‡ä» full ä¸­æŠ½æ ·è·å¾—ï¼Œç”¨äºæœ¬åœ°å¿«é€Ÿè°ƒè¯•ã€‚

ğŸ”¥ 5. è®­ç»ƒæµç¨‹
â–¶ 5.1 å•ä»»åŠ¡è®­ç»ƒï¼ˆéªŒè¯ DS-LoRAï¼‰
ç¤ºä¾‹ï¼šåœ¨ T1ï¼ˆé€šç”¨æŒ‡ä»¤ä»»åŠ¡ï¼‰ä¸Šè®­ç»ƒï¼š

bash
å¤åˆ¶ä»£ç 
python train_single_task.py --task T1_general
å…¶ä»–ä»»åŠ¡ï¼š

bash
å¤åˆ¶ä»£ç 
python train_single_task.py --task T2_math
python train_single_task.py --task T3_code
python train_single_task.py --task T4_tool
python train_single_task.py --task T5_safety
è¾“å‡ºï¼š

è®­ç»ƒæ—¥å¿—ï¼šlogs/single_<task>_train_loss.jsonl

è¯„ä¼°æ—¥å¿—ï¼šlogs/single_<task>_eval_loss.jsonl

æ¨¡å‹ï¼šcheckpoints/single_<task>_epochX/

â–¶ 5.2 æŒç»­ä»»åŠ¡åºåˆ—è®­ç»ƒï¼ˆDS-LoRA + SLSDï¼‰
bash
å¤åˆ¶ä»£ç 
python train_slsd_seq.py
è®­ç»ƒé¡ºåºå›ºå®šä¸ºï¼šT1 â†’ T2 â†’ T3 â†’ T4 â†’ T5

è¾“å‡ºï¼š

è¯„ä¼°æ—¥å¿—ï¼šlogs/seq_eval_loss.jsonl

é˜¶æ®µæ¨¡å‹ï¼š

checkpoints/seq_T1_general/

checkpoints/seq_T2_math/

...

checkpoints/seq_T5_safety/

ğŸ“ˆ 6. å¯è§†åŒ–
å•ä»»åŠ¡ loss æ›²çº¿ï¼š

bash
å¤åˆ¶ä»£ç 
python plot_loss.py single T1_general
# è¾“å‡ºï¼šlogs/single_T1_general_train_loss.png
åºåˆ—ä»»åŠ¡å¤šä»»åŠ¡è¯„ä¼°æ›²çº¿ï¼š

bash
å¤åˆ¶ä»£ç 
python plot_loss.py seq
# è¾“å‡ºï¼šlogs/seq_eval_loss.png
â— 7. å¸¸è§é—®é¢˜ï¼ˆFAQï¼‰
7.1 Loss å‡ºç° NaNï¼Ÿ
ä½¿ç”¨ float16 æ—¶å¯èƒ½å‡ºç°æ•°å€¼ä¸ç¨³å®šï¼Œå»ºè®®ï¼š

é™ä½å­¦ä¹ ç‡ï¼ˆå·²ç»è®¾ä¸º lr_slow = 1e-5, lr_fast = 5e-5ï¼‰

å‡å° max_seq_len æˆ– batch size

å¦‚ä»å‡ºç°ï¼š

åœ¨éœ€è¦æ—¶æ”¹ä¸º torch_dtype=torch.float32ï¼ˆä¼šæ›´æ…¢ä½†æ›´ç¨³ï¼‰

7.2 CUDA OOMï¼Ÿ
å‡å°ï¼š

per_device_batch_size

max_seq_len

å¢å¤§ï¼š

gradient_accumulation_steps

å¦‚æœæ˜¾å­˜ä»ä¸å¤Ÿï¼š

è€ƒè™‘ 4-bit / 8-bit é‡åŒ–åŠ è½½ LLaMA

ğŸ™ 8. è‡´è°¢
æœ¬é¡¹ç›®åŸºäºä»¥ä¸‹å¼€æºå·¥ä½œå’Œå·¥å…·ï¼š

HuggingFace Transformers / Datasets
HuggingFace Transformers / Datasets

PyTorch

LoRA (Hu et al.)

å„å¼€æ”¾æŒ‡ä»¤æ•°æ®é›†ï¼ˆAlpaca, Dolly, GSM8K, CodeAlpaca, HH-RLHF, function-calling æ•°æ®ç­‰ï¼‰

ä»¥åŠä½ çš„ç¡•å£«è®ºæ–‡ï¼šDS-LoRA + SLSD æŒç»­æŒ‡ä»¤å¾®è°ƒæ¡†æ¶

æ¬¢è¿å¼•ç”¨ã€æ‰©å±•ä¸æ”¹è¿›æœ¬é¡¹ç›®ã€‚